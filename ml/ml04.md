# 机器学习 [04] 卡卡利科

无论是频率学派的方法还是贝叶斯学派的方法，解决的都是怎么学的问题。但对于一个给定的问题到底能够学到什么程度，还需要专门的**计算学习理论**来解释。

## 计算学习理论

虽然不能对每个特定问题给出最优解，但概率论可以用来指导通用学习问题的求解，从而给出一些基本原则。

在概率论中，有界的独立随机变量的求和结果与求和数学期望的偏离程度存在一个固定的上界，这一关系可以用 Hoeffding 不等式来表示。

**Hoeffding 不等式能够说明很多问题：**

1. 用随机变量(样本中计算出的) v 来估计未知参数 u 时，虽然前者的概率分布在一定程度取决于后者，但估计的精度只和样本容量 N 有关
2. 想要提高估计的精度，最本质的方法还是增加样本容量，也就是多采一些数据，当总体的所有数据都被采样时，估计值也就完全等于真实值了。反过来说，只要样本容量足够大，估计值和真实值的差值将会以较大的概率被限定在较小的常数 e 之内。

经过推广后，Hoeffding 不等式就变成了对单个模型在训练集上的错误概率和在所有数据上的错误概率之间关系的描述，也就是**训练误差和泛化误差的关系**。它说明总会存在一个足够大的样本容量 N 使两者近似相等，这时就可以根据模型的训练误差来推导其泛化误差，从而获得关于真实情况的一些信息。

当训练误差 v 接近于 0 时，与之接近的泛化误差 u 也会接近于 0 ，据此可以推断出模型在整个的输入空间都能够以较大的概率逼近真实情况。

**让模型取得较小的泛化误差可以分成两步：**

1. 让训练误差足够小
2. 让泛化误差和训练误差足够接近

正是这种思路催生了机器学习中的“概率近似正确”（PAC）学习理论，它是一套用来对机器学习进行数学分析的理论框架。在这个框架下，**机器学习利用训练集来选择出的模型很可能（对应名称中的“概率”）具有较低额泛化误差（对应名称中的“近似正确”）**

**样本复杂度**是保证一个概率近似正确解所需要的样本数量。可以证明，所有假设空间有限的问题都是 PAC 可学习的，其样本复杂度有固定的下界，输出假设的泛化误差会随着样本数目的增加以一定速度收敛到 0。

**VC维**是对无限假设空间复杂度的一种度量方式，也可以用于给出模型泛化误差在概率意义上的上界。

任何 VC 维有限的假设空间都是 PAC 可学习的。

较小的 VC 维虽然能够让训练误差和泛化误差更加接近，但这样的假设空间不具备较强的表达能力，训练误差本身难以降低。反过来，VC 维更大的假设空间表达能力更强，得到的训练误差也会更小，但训练误差下降所付出的代价是训练误差和泛化误差之间更可能出现较大的差异，训练集上较小的误差不能推广到未知数据上。这其实也体现了模型复杂度和泛化性能之间的折中关系。

## 模型的分类方式 

机器学习学的是输入和输出之间的映射关系，学到的映射会以模型的形式出现。

大多数情况下，机器学习的任务是求解输入输出单独或者共同符合的概率分布，或者拟合输入输出之间的数量关系。

从数据的角度看，如果待求解的概率分布或者数量关系可以用一组有限且固定数目的参数完全刻画，求出的模型就是参数模型；反过来，不满足这个条件的模型就是非参数模型。

参数模型的优点在于只用少量参数就完整地描述出数据的概率特性，参数集中的每个参数都具有明确的统计意义。

**完全不使用先验信息，完全依赖数据进行学习得到的模型就是非参数模型。**

**非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画。**

和参数相比，非参数模型的时空复杂度都会被参数模型大得多。但可以证明的是，当训练数据趋于无穷多时，非参数模型可以逼近任意复杂的真实模型。

非参数模型可以理解为一种局部模型，参数模型具有全局的特性。

数据模型认为这个黑盒里装着一组未知的参数，学习的对象是这组参数；算法模型则认为这个黑盒里装着一个未知的映射，学习的对象也是这个映射。

如果说参数模型与非参数模型的核心区别在于数据分布特征的整体性和局部性，那么数据模型和算法模型之间的矛盾就是模型的可解释性和精确性的矛盾。

数据模型最典型的方法就是线性回归，也就是将输出结果表示为输入特征的线性加权组合，算法通过训练数据来学习加权系数。

随机森林是一种集成学习方法，构成这座森林的每一棵树都是决策树，每一棵决策树都用随机选取数据和待选特征构造出来的，再按照少数服从多数的原则从所有决策树的结果中得到最终输出。

决策树本身是具有较好可解释性的数据模型，它表示的是几何意义上对特征空间的划分，但是精度却不甚理想。随机森林解决了这个问题：通过综合使用建立在同一个数据集上的不同决策树达到出人意料的良好效果，在很多问题上都将精度提升了数倍。但精确度的提升换来的是可解释性的下降。

**生成模型**学习的对象是输入 x 和输出 y 的联合分布 p(x,y)，**判别模型**学习的是已知输入 x 的条件，输出 y 的条件分布 p(y|X)。两个定理可以通过贝叶斯定理建立联系。

## 又到了我第七喜欢的数学知识环节

数学期望是什么...

### 数学期望

在概率论和统计学中，数学期望（mean）(或均值，亦简称期望)是试验中每次可能结果的概率乘以其结果的总和，是最基本的数学特征之一。它反映随机变量平均取值大小。

**性质**

设 C 为一个常数，X 和 Y 是两个随机变量。以下是数学期望的重要性质：

1. E( C ) = C
2. E(CX) = CE(X)
3. E(X + Y) = E(X) + E(Y)
4. 当 X 和 Y 相互独立时， E(EY) = E(X)E(Y)

性质 3 和性质 4 可以推到任意有限个相互独立的随机变量之和或之积的情况。