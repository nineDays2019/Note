# 机器学习 [03] 瞭望塔

From 《极客时间|机器学习 40 讲》	(02 贝叶斯视角下的机器学习)

贝叶斯学派给出了一种更加通用的概率定义：**概率表示的是客观上事件的可信程度，也可以说是主观上主体对事件的信任程度，它是建立在对已有知识基础上的。**

**贝叶斯定理的意义正是在于将先验概率和后验概率关联起来，刻画了数据对于知识和信念的影响。**

## 该死的数学知识

继续是统计和概率...

### 贝叶斯定理

贝叶斯定理是概率论中的一个定理，它跟随机变量的条件概率以及边缘概率分布有关。

通常，事件 A 在事件 B 的条件下的概率，与事件 B 在事件 A 的条件下的概率是不一样的。然而，这两者是有确定的关系的，贝叶斯定理就是这种关系的陈述。

贝叶斯公式的一个用途在于通过已知的三个概率函数推出第四个。

#### 陈述

条件概率： `P(A|B) = P(A,B) / P(B)` 

P(A|B) 表示在事件 B 发生的条件下，事件 A 发生的条件概率

P(A,B) 表示 A 和 B 同时发生的联合概率

贝叶斯定理是关于随机事件 A 和 B 的条件概率的一则定理。

`P(A|B) = P(A) * P(B|A) / P(B)`

其中 P(A|B) 是指在事件 B 发生的情况下事件 A 发生的概率。

在贝叶斯定理中，每个名词都有约定俗成的名称：

* P(A|B) 是已知 B 发生后 A 的条件概率，也由于得自 B 的取值而称作 A 的*后验概率*
* P(A) 是 A 的*先验概率* （或*边缘概率*）。之所以称为“先验”是因为它不考虑任何 B 方面的因素。
* P(B|A) 是已知 A 发生后 B 的条件概率，也由于得自 A 的取值而称作 B 的 *后验概率*
* P(B) 是 B 的*先验概率*或*边缘概率*

按照这些术语，贝叶斯定理可表述为：

**后验概率 = （似然性 * 先验概率）/ 标准化常量**

也就是说，后验概率与先验概率和相似度的乘积成正比。

贝叶斯定理通常可以再写成下面的形式：

`P(B) = P(A,B) + P(Ac,B) = P(B|A)P(A) + P(B|Ac)P(Ac)`

其中 Ac 是 A 的补集（即非 A ），所以上式可写成：

`P(A|B) = P(B|A)P(A) / ( P(B|A)P(A) + P(B|Ac)P(Ac) )`


## 认知

* 贝叶斯学派认为概率是事件的可信程度或主体对时间的信任程度
* 贝叶斯学派执行参数估计时，视参数为随机变量，视数据为确定取值
* 贝叶斯学派主要使用最大后验概率法，让参数在先验信息和给定数据下的后验概率最大化
* 贝叶斯学派对应机器学习中的概率图模型，可以在模型预测和选择中提供更加完整的信息

什么样的问题才能通过机器学习来解决呢？

1. 问题不能是完全随机的，需要具备一定的模式
2. 问题本身不能通过纯计算的方法解决
3. 有大量的数据可供使用


机器学习的任务，就是使用数据计算出与目标函数最接近的假设，或者说拟合出最精确的模型。

监督学习适用于预测任务，无监督学习适用于描述任务。
